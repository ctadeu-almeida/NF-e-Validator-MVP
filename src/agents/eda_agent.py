# -*- coding: utf-8 -*-
"""
EDA Agent - Exploratory Data Analysis Agent

This module contains the main EDA agent that can analyze CSV files and answer questions
about data using various analysis techniques and visualizations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, List, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import tool
from langchain.schema.output_parser import StrOutputParser
import io
import sys
from contextlib import redirect_stdout, redirect_stderr
# offline_analyzer removido - n√£o mais necess√°rio

class EDAAgent:
    """
    Main EDA Agent class that provides comprehensive data analysis capabilities
    """

    def __init__(self, model_type: str = "gemini", api_key: str = None):
        """
        Initialize the EDA Agent with Gemini AI model

        IMPORTANTE: Apenas Gemini √© suportado. Agentes s√£o obrigat√≥rios.

        Args:
            model_type (str): "gemini" - √∫nico modelo suportado
            api_key (str): API key do Google (obrigat√≥ria)
        """
        if model_type != "gemini":
            raise ValueError("Apenas 'gemini' √© suportado.")

        self.model_type = model_type
        self.api_key = api_key
        self.data = None
        self.filename = None
        self.offline_analyzer = None  # Mantido apenas para compatibilidade de tools
        self.llm = None
        self.api_available = False
        self.chart_callback = None

        # Sistema de mem√≥ria expandido para conclus√µes e contexto
        self.analysis_history = []  # Hist√≥rico de an√°lises realizadas
        self.conclusions_memory = []  # Conclus√µes extra√≠das
        self.session_context = {  # Contexto da sess√£o atual
            'dataset_summary': None,
            'key_findings': [],
            'analysis_count': 0
        }

        # Initialize based on model type - APENAS AGENTES
        if model_type == "gemini" and api_key:
            self._init_gemini(api_key)
        else:
            raise ValueError(f"Configura√ß√£o inv√°lida para {model_type}. API key do Gemini √© obrigat√≥ria.")

        # SEMPRE inicializar agentes quando API estiver dispon√≠vel
        if self.api_available:
            print(f"‚úÖ {model_type.title()} inicializado - configurando agentes...")
            # Mem√≥ria aprimorada para manter contexto de an√°lises
            from langchain.memory import ConversationSummaryBufferMemory

            self.memory = ConversationSummaryBufferMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True,
                input_key="input",
                max_token_limit=2000,  # Manter hist√≥rico substancial
                summary_message_cls=None  # Usar mensagens padr√£o
            )
            self.tools = self._create_tools()
            self.agent_executor = self._create_agent()
            print(f"ü§ñ Agentes configurados para {model_type.title()}")
        else:
            raise RuntimeError(f"Falha ao inicializar {model_type.title()}. Agentes s√£o obrigat√≥rios.")

    def _init_gemini(self, api_key: str):
        """Initialize Google Gemini"""
        try:
            # Tentar modelos Gemini 2.5 dispon√≠veis em ordem de prioridade
            models_to_try = [
                'gemini-2.5-flash',
                'gemini-2.5-pro',
                'gemini-2.0-flash-exp'
            ]

            for model_name in models_to_try:
                try:
                    print(f"üîÑ Tentando inicializar {model_name}...")
                    self.llm = ChatGoogleGenerativeAI(
                        temperature=0.1,
                        model=model_name,
                        google_api_key=api_key,
                        max_output_tokens=4096,  # Valor mais conservador
                        convert_system_message_to_human=True,
                        max_retries=2,  # Reduzido para evitar loops
                        request_timeout=120,  # Timeout aumentado para 2 min
                        streaming=False  # Desabilitar streaming que pode causar problemas
                    )

                    # Testar inicializa√ß√£o com uma pergunta simples
                    test_response = self.llm.invoke([{"role": "user", "content": "Teste: responda apenas 'OK'"}])
                    if test_response and test_response.content:
                        self.api_available = True
                        print(f"‚úÖ Google Gemini inicializado com sucesso: {model_name}")
                        return
                    else:
                        print(f"‚ö†Ô∏è {model_name} retornou resposta vazia")

                except Exception as model_error:
                    print(f"‚ùå Falha com {model_name}: {model_error}")
                    continue

            # Se chegou aqui, nenhum modelo funcionou
            raise Exception("Nenhum modelo Gemini dispon√≠vel funcionou")

        except Exception as e:
            print(f"‚ùå Falha ao inicializar Google Gemini: {e}")
            self.api_available = False


    def load_data(self, file_path: str) -> bool:
        """
        Load CSV data for analysis

        Args:
            file_path (str): Path to CSV file

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Try different encodings and separators for better compatibility
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            separators = [',', ';', '\t']

            self.data = None
            load_error = None

            for encoding in encodings:
                for sep in separators:
                    # Try multiple reading strategies
                    reading_strategies = [
                        # Strategy 1: Handle malformed CSV with nested quotes
                        {
                            'encoding': encoding,
                            'sep': sep,
                            'low_memory': False,
                            'quoting': 1,  # QUOTE_ALL
                            'doublequote': True,
                            'skipinitialspace': True
                        },
                        # Strategy 2: Standard reading
                        {
                            'encoding': encoding,
                            'sep': sep,
                            'low_memory': False,
                            'quotechar': '"',
                            'skipinitialspace': True
                        },
                        # Strategy 3: No quote handling
                        {
                            'encoding': encoding,
                            'sep': sep,
                            'low_memory': False,
                            'quoting': 3  # QUOTE_NONE
                        },
                        # Strategy 4: Alternative quote character
                        {
                            'encoding': encoding,
                            'sep': sep,
                            'low_memory': False,
                            'quotechar': "'",
                            'skipinitialspace': True
                        }
                    ]

                    for strategy_idx, params in enumerate(reading_strategies):
                        try:
                            self.data = pd.read_csv(file_path, **params)
                            if self.data.shape[1] > 1:  # Valid if has more than 1 column
                                strategy_name = f"estrat√©gia {strategy_idx + 1}"
                                print(f"‚úÖ Arquivo carregado com encoding '{encoding}', separador '{sep}' ({strategy_name})")
                                break
                        except Exception as e:
                            load_error = e
                            continue

                    if self.data is not None and self.data.shape[1] > 1:
                        break
                if self.data is not None and self.data.shape[1] > 1:
                    break

            # If still only 1 column, try to fix malformed CSV
            if self.data is not None and self.data.shape[1] == 1:
                self.data = self._fix_malformed_csv(file_path)

            if self.data is None or self.data.shape[1] <= 1:
                raise Exception(f"N√£o foi poss√≠vel carregar o arquivo. √öltimo erro: {load_error}")

            self.filename = file_path.split('\\')[-1] if '\\' in file_path else file_path.split('/')[-1]

            # Clean problematic quotes first
            self.data = self._clean_quotes_from_data(self.data)

            # Apply general preprocessing to any CSV file
            self.data = self._preprocess_csv_data(self.data)

            # Initialize offline analyzer
            # offline_analyzer removido

            return True
        except Exception as e:
            print(f"‚ùå Erro ao carregar dados: {e}")
            print("üí° Dicas:")
            print("   - Verifique se o arquivo n√£o est√° aberto em outro programa")
            print("   - Confirme se voc√™ tem permiss√£o para acessar o arquivo")
            print("   - Verifique se o caminho est√° correto")
            return False

    def _fix_malformed_csv(self, file_path: str) -> pd.DataFrame:
        """
        Fix malformed CSV files with nested quotes

        Args:
            file_path (str): Path to the malformed CSV file

        Returns:
            pd.DataFrame: Fixed data
        """
        print("üîß Corrigindo formato CSV mal formado...")

        try:
            # Read the raw file and fix the format
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            fixed_lines = []
            for line in lines:
                # Fix the malformed header and data
                if line.startswith('"Time,'):
                    # Fix header line: "Time,""V1"",""V2""... to Time,V1,V2...
                    fixed_line = line.strip().strip('"')  # Remove outer quotes
                    fixed_line = fixed_line.replace('""', '"')  # Fix double quotes
                    fixed_line = fixed_line.replace('","', ',').replace('",', ',').replace(',"', ',')
                    fixed_line = fixed_line.replace('"', '')  # Remove all remaining quotes
                    fixed_lines.append(fixed_line + '\n')
                else:
                    # Fix data lines
                    line = line.strip()
                    if '""' in line:
                        # Handle lines with pattern: value,""value"",""value""...
                        parts = []
                        in_quotes = False
                        current_part = ""
                        i = 0
                        while i < len(line):
                            if line[i:i+2] == '""':
                                if current_part:
                                    parts.append(current_part)
                                    current_part = ""
                                i += 2
                                continue
                            elif line[i] == '"':
                                i += 1
                                continue
                            elif line[i] == ',':
                                if current_part:
                                    parts.append(current_part)
                                    current_part = ""
                                i += 1
                                continue
                            current_part += line[i]
                            i += 1

                        if current_part:
                            parts.append(current_part)

                        fixed_lines.append(','.join(parts) + '\n')
                    else:
                        fixed_lines.append(line + '\n')

            # Create a temporary file-like object from fixed content
            from io import StringIO
            fixed_content = ''.join(fixed_lines)
            fixed_file = StringIO(fixed_content)

            # Read the fixed CSV
            fixed_data = pd.read_csv(fixed_file)
            print(f"   ‚úì CSV corrigido: {fixed_data.shape[1]} colunas")

            return fixed_data

        except Exception as e:
            print(f"   ‚ùå Erro ao corrigir CSV: {e}")
            return None

    def _clean_quotes_from_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Clean problematic quotes from CSV data that can interfere with processing

        Args:
            data (pd.DataFrame): Raw data with potential quote issues

        Returns:
            pd.DataFrame: Data with quotes cleaned
        """
        print("üßπ Limpando aspas...")

        cleaned_data = data.copy()

        # Clean column names
        cleaned_data.columns = [col.strip().strip('"').strip("'") for col in cleaned_data.columns]

        # 2. Clean data values in object columns
        object_cols = cleaned_data.select_dtypes(include=['object']).columns
        cleaned_values_count = 0

        for col in object_cols:
            # Check if column has problematic quotes
            sample_values = cleaned_data[col].astype(str).head(10)
            has_quotes = any('"' in str(val) for val in sample_values)

            if has_quotes:
                try:
                    # Clean quotes from the entire column
                    original_series = cleaned_data[col].astype(str)

                    # Remove leading/trailing quotes and clean internal quotes
                    cleaned_series = original_series.str.strip()
                    cleaned_series = cleaned_series.str.strip('"').str.strip("'")

                    # Handle double quotes that might be escaped
                    cleaned_series = cleaned_series.str.replace('""', '"')
                    cleaned_series = cleaned_series.str.replace("''", "'")

                    # Convert back to original type if it was numeric
                    try:
                        # Test if the cleaned version can be numeric
                        numeric_test = pd.to_numeric(cleaned_series, errors='coerce')
                        if numeric_test.notna().sum() / len(numeric_test) > 0.8:
                            cleaned_data[col] = numeric_test
                            cleaned_values_count += 1
                        else:
                            cleaned_data[col] = cleaned_series
                            cleaned_values_count += 1
                    except:
                        cleaned_data[col] = cleaned_series
                        cleaned_values_count += 1

                except:
                    pass

        if cleaned_values_count > 0:
            print(f"   ‚úì {cleaned_values_count} colunas processadas")

        # Additional cleaning
        for col in cleaned_data.columns:
            if cleaned_data[col].dtype == 'object':
                try:
                    cleaned_data[col] = cleaned_data[col].astype(str).str.replace('\ufeff', '').str.strip()
                    cleaned_data[col] = cleaned_data[col].replace(['nan', 'NaN', 'NULL', 'null', ''], pd.NA)
                except:
                    pass

        print("‚úÖ Aspas limpas!")
        print()

        return cleaned_data

    def _preprocess_teste_csv(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Preprocess the teste.csv file with specific treatments for fraud detection data

        Args:
            data (pd.DataFrame): Raw data from teste.csv

        Returns:
            pd.DataFrame: Preprocessed data
        """
        print("üìã Aplicando tratamentos espec√≠ficos para dados de fraude...")

        # Create a copy to avoid modifying original
        processed_data = data.copy()

        # 1. Ensure proper column names and types
        expected_columns = ['Time'] + [f'V{i}' for i in range(1, 29)] + ['Amount', 'Class']

        # 2. Convert Class to categorical with proper labels
        if 'Class' in processed_data.columns:
            processed_data['Class'] = processed_data['Class'].astype(int)
            processed_data['Class_Label'] = processed_data['Class'].map({0: 'Normal', 1: 'Fraudulenta'})
            print(f"   ‚úì Classes identificadas: {processed_data['Class'].value_counts().to_dict()}")

        # 3. Ensure Time is properly formatted
        if 'Time' in processed_data.columns:
            processed_data['Time'] = pd.to_numeric(processed_data['Time'], errors='coerce')
            print(f"   ‚úì Tempo: {processed_data['Time'].min():.2f}s a {processed_data['Time'].max():.2f}s")

        # 4. Ensure Amount is properly formatted
        if 'Amount' in processed_data.columns:
            processed_data['Amount'] = pd.to_numeric(processed_data['Amount'], errors='coerce')
            print(f"   ‚úì Valores: ${processed_data['Amount'].min():.2f} a ${processed_data['Amount'].max():.2f}")

        # 5. Ensure V1-V28 columns are numeric (PCA components)
        pca_columns = [col for col in processed_data.columns if col.startswith('V') and col[1:].isdigit()]
        for col in pca_columns:
            processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')

        print(f"   ‚úì {len(pca_columns)} componentes PCA processadas")

        # 6. Create additional derived features for fraud analysis
        if 'Amount' in processed_data.columns and 'Time' in processed_data.columns:
            # Hour of day (assuming Time is seconds from start)
            processed_data['Hour'] = ((processed_data['Time'] / 3600) % 24).astype(int)

            # Amount categories
            processed_data['Amount_Category'] = pd.cut(
                processed_data['Amount'],
                bins=[0, 10, 50, 100, 500, float('inf')],
                labels=['Muito_Baixo', 'Baixo', 'Medio', 'Alto', 'Muito_Alto']
            )

            print("   ‚úì Features derivadas criadas: Hour, Amount_Category")

        # 7. Check data quality
        missing_values = processed_data.isnull().sum().sum()
        if missing_values > 0:
            print(f"   ‚ö†Ô∏è {missing_values} valores faltantes encontrados")
        else:
            print("   ‚úì Nenhum valor faltante")

        # 8. Fraud analysis summary
        if 'Class' in processed_data.columns:
            fraud_rate = (processed_data['Class'] == 1).mean() * 100
            print(f"   üìä Taxa de fraude: {fraud_rate:.2f}%")

        print("‚úÖ Tratamento espec√≠fico do arquivo teste.csv conclu√≠do!")
        print()

        return processed_data

    def _preprocess_csv_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        General preprocessing for any CSV file

        Args:
            data (pd.DataFrame): Raw CSV data

        Returns:
            pd.DataFrame: Preprocessed data
        """
        print("üìã Processando dados...")

        processed_data = data.copy()

        # Clean column names
        processed_data.columns = processed_data.columns.str.strip().str.replace('"', '')

        # 2. Detect and convert numeric columns
        numeric_converted = 0
        for col in processed_data.columns:
            if processed_data[col].dtype == 'object':
                # Try to convert to numeric
                try:
                    # Remove common non-numeric characters and quotes
                    cleaned_series = processed_data[col].astype(str)
                    cleaned_series = cleaned_series.str.replace('"', '')  # Remove quotes
                    cleaned_series = cleaned_series.str.replace(',', '.')  # Handle decimal separator
                    cleaned_series = cleaned_series.str.replace(r'[^\d.-]', '', regex=True)

                    # Handle empty strings
                    cleaned_series = cleaned_series.replace('', '0')

                    numeric_series = pd.to_numeric(cleaned_series, errors='coerce')

                    # If most values are successfully converted, use numeric
                    successful_conversion_rate = numeric_series.notna().sum() / len(numeric_series)
                    if successful_conversion_rate > 0.8:
                        processed_data[col] = numeric_series
                        numeric_converted += 1
                except:
                    pass

        # Force conversion for expected numeric columns
        expected_numeric_cols = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]
        forced_conversions = 0
        for col in expected_numeric_cols:
            if col in processed_data.columns and processed_data[col].dtype == 'object':
                try:
                    cleaned_series = processed_data[col].astype(str).str.replace('"', '').str.replace(',', '.')
                    cleaned_series = cleaned_series.str.replace(r'[^\d.-]', '', regex=True).replace('', '0')
                    processed_data[col] = pd.to_numeric(cleaned_series, errors='coerce')
                    forced_conversions += 1
                except:
                    pass

        total_converted = numeric_converted + forced_conversions
        if total_converted > 0:
            print(f"   ‚úì {total_converted} colunas convertidas para num√©ricas")

        # Handle missing values and cleanup
        missing_total = processed_data.isnull().sum().sum()
        if missing_total > 0:
            print(f"   ‚ö†Ô∏è {missing_total} valores faltantes")

        # Remove empty rows/columns
        processed_data = processed_data.dropna(how='all').loc[:, ~processed_data.isnull().all()]

        # Final summary
        numeric_cols = processed_data.select_dtypes(include=[np.number]).columns
        categorical_cols = processed_data.select_dtypes(include=['object', 'category']).columns

        print(f"   üìä {len(numeric_cols)} num√©ricas, {len(categorical_cols)} categ√≥ricas, {len(processed_data)} registros")

        # Handle Class column for classification (don't add new column, just detect)
        if 'Class' in processed_data.columns and processed_data['Class'].dtype in ['int64', 'float64']:
            unique_classes = processed_data['Class'].unique()
            if len(unique_classes) <= 10:
                print(f"   üéØ Alvo de classifica√ß√£o detectado: {unique_classes}")

        print("‚úÖ Dados processados!")
        print()

        return processed_data

    def _create_tools(self) -> List[Tool]:
        """Create analysis tools for the agent"""

        @tool
        def describe_data() -> str:
            """Get basic information about the loaded dataset including shape, columns, data types"""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            info = []
            info.append(f"Dataset: {self.filename}")
            info.append(f"Shape: {self.data.shape[0]} rows, {self.data.shape[1]} columns")
            info.append(f"Columns: {list(self.data.columns)}")
            info.append(f"Data types:\n{self.data.dtypes.to_string()}")
            info.append(f"Missing values:\n{self.data.isnull().sum().to_string()}")

            return "\n\n".join(info)

        @tool
        def get_statistical_summary() -> str:
            """Get statistical summary of numerical columns"""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            numeric_cols = self.data.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) == 0:
                return "No numerical columns found in the dataset."

            summary = self.data[numeric_cols].describe()
            return f"Statistical Summary:\n{summary.to_string()}"

        @tool
        def check_correlations() -> str:
            """Calculate and return correlation matrix for numerical columns"""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            numeric_cols = self.data.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) < 2:
                return "Need at least 2 numerical columns to calculate correlations."

            corr_matrix = self.data[numeric_cols].corr()
            return f"Correlation Matrix:\n{corr_matrix.to_string()}"

        @tool
        def detect_outliers(column: str = None) -> str:
            """Detect outliers using IQR method for a specific column or all numerical columns"""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            numeric_cols = self.data.select_dtypes(include=[np.number]).columns

            if column and column not in numeric_cols:
                return f"Column '{column}' not found or not numerical."

            cols_to_check = [column] if column else numeric_cols
            outlier_info = []

            for col in cols_to_check:
                Q1 = self.data[col].quantile(0.25)
                Q3 = self.data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                outliers = self.data[(self.data[col] < lower_bound) | (self.data[col] > upper_bound)]
                outlier_count = len(outliers)

                outlier_info.append(f"Column '{col}': {outlier_count} outliers detected")
                if outlier_count > 0:
                    outlier_info.append(f"  Range: {lower_bound:.2f} to {upper_bound:.2f}")
                    outlier_info.append(f"  Outlier values sample: {outliers[col].head().tolist()}")

            return "\n".join(outlier_info)

        @tool
        def analyze_categorical_data() -> str:
            """Analyze categorical columns including value counts and unique values"""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns

            if len(categorical_cols) == 0:
                return "No categorical columns found in the dataset."

            cat_info = []
            for col in categorical_cols:
                unique_count = self.data[col].nunique()
                cat_info.append(f"Column '{col}':")
                cat_info.append(f"  Unique values: {unique_count}")
                cat_info.append(f"  Value counts:\n{self.data[col].value_counts().head().to_string()}")
                cat_info.append("")

            return "\n".join(cat_info)

        @tool
        def generate_and_execute_python_code(question: str) -> str:
            """Generate and execute Python code to answer the user's question about the data."""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            # Create a safe execution environment
            # Criar pasta charts se n√£o existir
            import os
            import datetime
            charts_dir = 'charts'
            if not os.path.exists(charts_dir):
                os.makedirs(charts_dir)

            # Gerar timestamp √∫nico para arquivos
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

            # Verificar se self.data existe
            if self.data is None:
                return "Erro: Nenhum dataset foi carregado. Use load_data() primeiro."

            local_vars = {
                'data': self.data,
                'pd': pd,
                'np': np,
                'plt': plt,
                'sns': sns,
                'os': os,
                'timestamp': timestamp
            }

            # Debug: verificar se vari√°veis est√£o sendo passadas
            print(f"üîç Debug: data shape = {self.data.shape}, vari√°veis passadas = {list(local_vars.keys())}")

            # Ask the LLM to generate Python code for the specific question
            code_generation_prompt = f"""
            Voc√™ tem um dataset pandas chamado 'data' carregado. O usu√°rio fez a seguinte pergunta:
            "{question}"

            Gere c√≥digo Python que responda especificamente a esta pergunta. O c√≥digo deve:
            1. Analisar os dados relevantes e fazer c√°lculos necess√°rios
            2. Imprimir resultados claros e informativos usando print()
            3. Gerar gr√°ficos relevantes quando apropriado usando matplotlib/seaborn
            4. Salvar gr√°ficos como PNG na pasta charts/ se criados
            5. Tirar conclus√µes baseadas nos dados analisados

            VARI√ÅVEIS DISPON√çVEIS:
            - data: DataFrame principal com os dados ({self.data.shape[0]} linhas, {self.data.shape[1]} colunas)
            - pd: pandas
            - np: numpy
            - plt: matplotlib.pyplot
            - sns: seaborn
            - os: os module
            - timestamp: string √∫nica para nomes de arquivo

            Dataset info:
            - Shape: {self.data.shape}
            - Columns: {list(self.data.columns)}
            - Data types: {dict(self.data.dtypes)}

            EXEMPLO DE C√ìDIGO V√ÅLIDO:
            ```python
            # Verificar se data est√° carregado
            print(f"Dataset shape: {{data.shape}}")
            print(f"Columns: {{list(data.columns)}}")

            # Gerar an√°lise
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            for col in numeric_cols[:3]:  # Primeiras 3 colunas
                plt.figure(figsize=(10, 6))
                data[col].hist(bins=30)
                plt.title(f'Distribui√ß√£o de {{col}}')
                plt.tight_layout()
                plt.savefig(f'charts/{{timestamp}}_hist_{{col}}.png', dpi=300, bbox_inches='tight')
                plt.close()
                print(f"Gr√°fico salvo: charts/{{timestamp}}_hist_{{col}}.png")
            ```

            IMPORTANTE:
            - Use SEMPRE 'data' como nome do DataFrame
            - SEMPRE feche figuras com plt.close()
            - Use timestamp para nomes √∫nicos de arquivo

            Retorne APENAS o c√≥digo Python, sem explica√ß√µes adicionais.
            """

            try:
                # Generate code using the LLM
                code_response = self.llm.invoke([{"role": "user", "content": code_generation_prompt}])
                generated_code = code_response.content.strip()

                # Clean the code (remove markdown formatting if present)
                if generated_code.startswith("```python"):
                    generated_code = generated_code[9:]
                if generated_code.startswith("```"):
                    generated_code = generated_code[3:]
                if generated_code.endswith("```"):
                    generated_code = generated_code[:-3]

                # Capture output
                output_buffer = io.StringIO()
                error_buffer = io.StringIO()

                # Execute the generated code
                with redirect_stdout(output_buffer), redirect_stderr(error_buffer):
                    exec(generated_code, {"__builtins__": __builtins__}, local_vars)

                output = output_buffer.getvalue()
                errors = error_buffer.getvalue()

                result = [f"C√≥digo gerado:\n{generated_code}\n"]
                result.append("="*50)

                if output:
                    result.append(f"Resultado:\n{output}")
                if errors:
                    result.append(f"Avisos/Erros:\n{errors}")

                # Sempre incluir uma conclus√£o interpretativa
                conclusion_prompt = f"""
                Baseado na pergunta: "{question}"
                E no resultado da execu√ß√£o do c√≥digo:
                {output[:1000] if output else "Sem output textual"}

                Forne√ßa uma conclus√£o interpretativa clara em 2-3 frases sobre o que estes resultados revelam sobre os dados.
                Seja espec√≠fico e direto.
                """

                try:
                    conclusion_response = self.llm.invoke([{"role": "user", "content": conclusion_prompt}])
                    conclusion = conclusion_response.content.strip()
                    if conclusion:
                        result.append(f"\nüîç Conclus√£o:\n{conclusion}")

                        # Armazenar conclus√£o na mem√≥ria da sess√£o
                        self._store_analysis_conclusion(question, conclusion, output)
                except:
                    fallback_conclusion = f"An√°lise executada com sucesso para: {question}"
                    result.append(f"\nüîç {fallback_conclusion}")
                    self._store_analysis_conclusion(question, fallback_conclusion, output)

                return "\n".join(result) if result else "Code executed successfully with no output."

            except Exception as e:
                return f"Error generating or executing code: {str(e)}"

        @tool
        def get_temporal_analysis() -> str:
            """Analyze temporal patterns if Time column exists"""
            if self.data is None:
                return "No data loaded. Please load a CSV file first."

            if 'Time' not in self.data.columns:
                return "No 'Time' column found in the dataset."

            time_info = []
            time_info.append(f"Time column analysis:")
            time_info.append(f"  Min time: {self.data['Time'].min()}")
            time_info.append(f"  Max time: {self.data['Time'].max()}")
            time_info.append(f"  Time range: {self.data['Time'].max() - self.data['Time'].min()} seconds")

            # Check for patterns
            time_diff = self.data['Time'].diff().dropna()
            time_info.append(f"  Average time between transactions: {time_diff.mean():.2f} seconds")

            return "\n".join(time_info)

        @tool
        def get_previous_conclusions() -> str:
            """Get conclusions from previous analyses in this session"""
            if not self.conclusions_memory:
                return "No previous analyses have been performed in this session."

            conclusions_summary = []
            conclusions_summary.append("CONCLUS√ïES DE AN√ÅLISES ANTERIORES:")
            conclusions_summary.append("=" * 50)

            for i, conclusion in enumerate(self.conclusions_memory[-5:], 1):  # √öltimas 5
                conclusions_summary.append(f"\n{i}. An√°lise: {conclusion['analysis_type']}")
                conclusions_summary.append(f"   Pergunta: {conclusion['question']}")
                conclusions_summary.append(f"   Conclus√£o: {conclusion['conclusion']}")
                if conclusion.get('key_findings'):
                    conclusions_summary.append(f"   Descobertas: {conclusion['key_findings']}")

            return "\n".join(conclusions_summary)

        @tool
        def get_session_context() -> str:
            """Get current session context including dataset summary and key findings"""
            context_info = []
            context_info.append("CONTEXTO DA SESS√ÉO ATUAL:")
            context_info.append("=" * 50)

            if self.session_context['dataset_summary']:
                context_info.append(f"Dataset: {self.session_context['dataset_summary']}")

            context_info.append(f"An√°lises realizadas: {self.session_context['analysis_count']}")

            if self.session_context['key_findings']:
                context_info.append("\nPrincipais descobertas:")
                for finding in self.session_context['key_findings']:
                    context_info.append(f"  ‚Ä¢ {finding}")

            if self.analysis_history:
                context_info.append(f"\nTipos de an√°lises j√° realizadas:")
                analysis_types = [a['type'] for a in self.analysis_history]
                unique_types = list(set(analysis_types))
                context_info.append(f"  {', '.join(unique_types)}")

            return "\n".join(context_info)

        return [
            describe_data,
            get_statistical_summary,
            check_correlations,
            detect_outliers,
            analyze_categorical_data,
            generate_and_execute_python_code,
            get_temporal_analysis,
            get_previous_conclusions,
            get_session_context
        ]

    def _create_agent(self) -> AgentExecutor:
        """Create the main EDA agent"""
        from langchain.prompts import PromptTemplate

        prompt = PromptTemplate.from_template("""Voc√™ √© um analista de dados expert especializado em An√°lise Explorat√≥ria de Dados (EDA).
Seu objetivo √© responder perguntas sobre datasets CSV mantendo MEM√ìRIA e CONTEXTO das an√°lises anteriores.

MEM√ìRIA E CONTEXTO:
- SEMPRE consulte get_previous_conclusions antes de an√°lises para manter continuidade
- Use get_session_context para entender o que j√° foi descoberto
- Referencie an√°lises anteriores quando relevante
- Construa sobre descobertas pr√©vias ao inv√©s de repetir an√°lises

INSTRU√á√ïES IMPORTANTES:
1. CONSULTE MEM√ìRIA: Use get_previous_conclusions e get_session_context para contexto
2. AN√ÅLISES COMPLEXAS: Use generate_and_execute_python_code para c√°lculos
3. ESTRUTURA B√ÅSICA: Use describe_data para informa√ß√µes gerais
4. SEJA CONTEXTUAL: Referencie an√°lises anteriores e construa sobre elas
5. MENCIONE GR√ÅFICOS: Se gerar visualiza√ß√µes, informe que foram salvos em charts/

HIST√ìRICO DA CONVERSA:
{chat_history}

FERRAMENTAS DISPON√çVEIS:
{tools}

FORMATO DE RESPOSTA:
Question: a pergunta que voc√™ deve responder
Thought: pense sobre o contexto e an√°lises anteriores primeiro
Action: escolha uma ferramenta de [{tool_names}]
Action Input: entrada para a ferramenta
Observation: resultado da ferramenta
... (Thought/Action/Action Input/Observation pode repetir)
Thought: agora sei a resposta final considerando o contexto
Final Answer: resposta final que conecta com an√°lises anteriores quando relevante

ESTRAT√âGIA COM MEM√ìRIA:
1. SEMPRE comece consultando get_previous_conclusions se n√£o for a primeira pergunta
2. Use get_session_context para entender descobertas j√° feitas
3. Para an√°lises espec√≠ficas: use generate_and_execute_python_code
4. Conecte novas descobertas com conclus√µes anteriores

Comece!

Question: {input}
Thought:{agent_scratchpad}""")

        agent = create_react_agent(self.llm, self.tools, prompt)
        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            memory=self.memory,
            verbose=True,  # Ativar para debug
            handle_parsing_errors=True,
            return_intermediate_steps=True,  # Manter passos intermedi√°rios
            max_iterations=25,  # Aumentado de 15 para 25
            early_stopping_method="generate",  # Mudado de "force" para "generate"
            max_execution_time=300,  # Aumentado de 120 para 300 segundos (5 min)
        )

    def ask_question(self, question: str) -> str:
        """
        Ask a question about the loaded data

        Args:
            question (str): Question about the data

        Returns:
            str: Agent's response
        """
        if self.data is None:
            return "Please load a CSV file first using the load_data method."

        # APENAS AGENTES - sem fallback offline
        if not self.api_available or not self.agent_executor:
            return f"‚ùå Erro: Agente {self.model_type} n√£o est√° dispon√≠vel. Verifique a configura√ß√£o."

        try:
            print(f"ü§ñ Executando pergunta via agente {self.model_type.title()}...")

            # Proteger contra StopIteration com try-catch espec√≠fico
            try:
                response = self.agent_executor.invoke({"input": question})
            except StopIteration as stop_iter:
                # Converter StopIteration em RuntimeError conforme Python 3.7+
                print(f"‚ö†Ô∏è StopIteration capturado e convertido: {stop_iter}")
                raise RuntimeError(f"Agent generator completed unexpectedly: {stop_iter}")

            # Melhorar extra√ß√£o da resposta
            if isinstance(response, dict):
                # Verificar output principal
                if "output" in response and response["output"]:
                    final_answer = response["output"]
                # Verificar se h√° passos intermedi√°rios com resultados
                elif "intermediate_steps" in response and response["intermediate_steps"]:
                    # Extrair √∫ltimo resultado √∫til dos passos intermedi√°rios
                    last_step = response["intermediate_steps"][-1]
                    if len(last_step) > 1:
                        final_answer = f"Resultado da an√°lise:\n{last_step[1]}"
                    else:
                        final_answer = "An√°lise executada, mas resposta incompleta."
                else:
                    final_answer = "An√°lise conclu√≠da, mas sem resposta estruturada."

                # Adicionar informa√ß√µes dos passos intermedi√°rios se √∫til
                if "intermediate_steps" in response and len(response["intermediate_steps"]) > 1:
                    final_answer += f"\n\nüìä Passos executados: {len(response['intermediate_steps'])}"

                return final_answer
            else:
                return str(response) if response else "An√°lise executada sem resposta."

        except RuntimeError as e:
            error_msg = str(e)
            print(f"‚ùå RuntimeError no agente {self.model_type}: {error_msg[:100]}")

            # Tratamento espec√≠fico para StopIteration convertido
            if "generator" in error_msg.lower() or "stopiteration" in error_msg.lower():
                return f"‚ùå Erro de gerador no {self.model_type.title()}. Pode estar relacionado √† vers√£o do SDK.\n\nüí° Tente reformular a pergunta ou reinicialize o modelo."
            else:
                return f"‚ùå Erro no agente {self.model_type.title()}: {error_msg[:200]}\n\nüí° Tente uma pergunta mais simples ou reinicialize o modelo."

        except Exception as e:
            error_str = str(e)
            print(f"‚ùå Erro geral no agente {self.model_type}: {error_str[:100]}")

            # Tratamento espec√≠fico para "No generation chunks were returned"
            if "no generation chunks were returned" in error_str.lower():
                # Tentar usar ferramentas b√°sicas como fallback
                try:
                    print("üîÑ Tentando fallback com ferramentas b√°sicas...")
                    if "distribui√ß√£o" in question.lower() or "histograma" in question.lower():
                        stats_result = self._create_tools()[1]()  # get_statistical_summary
                        return f"üìä An√°lise b√°sica (fallback):\n{stats_result}\n\n‚ö†Ô∏è Nota: Erro no Gemini ('No generation chunks'), usando an√°lise simplificada."
                    else:
                        basic_info = self._create_tools()[0]()  # describe_data
                        return f"üìã Informa√ß√µes b√°sicas (fallback):\n{basic_info}\n\n‚ö†Ô∏è Nota: Erro no Gemini ('No generation chunks'), reformule a pergunta."
                except:
                    pass

                return f"‚ùå Erro no agente Gemini: No generation chunks were returned\n\nüí° Este erro pode ocorrer por:\n‚Ä¢ Conte√∫do bloqueado por filtros de seguran√ßa\n‚Ä¢ Pergunta muito complexa ou mal formatada\n‚Ä¢ Problemas tempor√°rios da API\n\nüîÑ Sugest√µes:\n‚Ä¢ Reformule a pergunta de forma mais simples\n‚Ä¢ Tente uma pergunta diferente\n‚Ä¢ Verifique sua chave da API\n‚Ä¢ Reinicialize o modelo"

            # Tratamento espec√≠fico de erros de API
            elif any(keyword in error_str.lower() for keyword in ['quota', 'exceeded', '429', 'rate limit']):
                return f"‚ö†Ô∏è Cota da API {self.model_type.title()} excedida.\n\nüí° Aguarde um momento e tente novamente."
            elif "stopiteration" in error_str.lower():
                return f"‚ùå Erro de iterador no {self.model_type.title()}. Vers√£o do SDK pode ser incompat√≠vel.\n\nüí° Tente reformular a pergunta ou verifique a vers√£o do google-generativeai."
            else:
                return f"‚ùå Erro no agente {self.model_type.title()}: {error_str[:200]}\n\nüí° Verifique sua configura√ß√£o ou tente novamente."

    # MODO OFFLINE REMOVIDO - Apenas m√©todos de an√°lise auxiliares mantidos

    def _analyze_data_types(self) -> str:
        """
        Analisa os tipos de dados de forma detalhada

        Returns:
            str: An√°lise detalhada dos tipos de dados
        """
        if self.data is None:
            return "Nenhum dado carregado."

        result = []
        result.append("AN√ÅLISE DE TIPOS DE DADOS")
        result.append("=" * 50)
        result.append(f"Dataset: {self.filename}")
        result.append(f"Shape: {self.data.shape[0]} linhas, {self.data.shape[1]} colunas")
        result.append("")

        # An√°lise por tipo
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns

        result.append("RESUMO POR TIPO:")
        result.append(f"- Colunas num√©ricas: {len(numeric_cols)}")
        result.append(f"- Colunas categ√≥ricas: {len(categorical_cols)}")
        result.append(f"- Colunas de data/hora: {len(datetime_cols)}")
        result.append("")

        # Detalhes das colunas num√©ricas
        if len(numeric_cols) > 0:
            result.append("COLUNAS NUM√âRICAS:")
            for col in numeric_cols:
                dtype = str(self.data[col].dtype)
                non_null = self.data[col].count()
                result.append(f"  - {col}: {dtype} ({non_null} valores v√°lidos)")

        # Detalhes das colunas categ√≥ricas
        if len(categorical_cols) > 0:
            result.append("")
            result.append("COLUNAS CATEG√ìRICAS:")
            for col in categorical_cols:
                unique_vals = self.data[col].nunique()
                non_null = self.data[col].count()
                result.append(f"  - {col}: object ({non_null} valores v√°lidos, {unique_vals} √∫nicos)")

        # Detalhes das colunas de data
        if len(datetime_cols) > 0:
            result.append("")
            result.append("COLUNAS DE DATA/HORA:")
            for col in datetime_cols:
                non_null = self.data[col].count()
                result.append(f"  - {col}: datetime64 ({non_null} valores v√°lidos)")

        # Informa√ß√µes adicionais
        result.append("")
        result.append("INFORMA√á√ïES ADICIONAIS:")
        result.append(f"- Total de valores ausentes: {self.data.isnull().sum().sum()}")
        result.append(f"- Uso de mem√≥ria: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

        return "\n".join(result)

    def _generate_simple_plots(self, chart_callback=None):
        """Gera gr√°ficos b√°sicos para an√°lise explorat√≥ria"""
        import os
        import matplotlib.pyplot as plt
        import seaborn as sns

        # Criar pasta charts se n√£o existir
        charts_dir = 'charts'
        if not os.path.exists(charts_dir):
            os.makedirs(charts_dir)

        # Configurar estilo
        plt.style.use('default')
        sns.set_palette("husl")

        generated_charts = []

        # 1. Histogramas para vari√°veis num√©ricas
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            n_cols = min(len(numeric_cols), 4)
            n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

            plt.figure(figsize=(15, 4 * n_rows))
            for i, col in enumerate(numeric_cols):
                plt.subplot(n_rows, n_cols, i + 1)
                self.data[col].hist(bins=30, alpha=0.7)
                plt.title(f'Distribui√ß√£o de {col}')
                plt.xlabel(col)
                plt.ylabel('Frequ√™ncia')

            plt.tight_layout()
            chart_name = 'distribuicoes_numericas'
            plt.savefig(f'{charts_dir}/{chart_name}.png', dpi=300, bbox_inches='tight')
            plt.close()
            generated_charts.append(chart_name)

        # 2. Matriz de correla√ß√£o (se tiver mais de 1 vari√°vel num√©rica)
        if len(numeric_cols) > 1:
            plt.figure(figsize=(10, 8))
            corr_matrix = self.data[numeric_cols].corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                       square=True, linewidths=0.5)
            plt.title('Matriz de Correla√ß√£o')
            plt.tight_layout()
            chart_name = 'matriz_correlacao'
            plt.savefig(f'{charts_dir}/{chart_name}.png', dpi=300, bbox_inches='tight')
            plt.close()
            generated_charts.append(chart_name)

        # 3. Gr√°ficos de barras para vari√°veis categ√≥ricas
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            n_cols = min(len(categorical_cols), 3)
            n_rows = (len(categorical_cols) + n_cols - 1) // n_cols

            plt.figure(figsize=(15, 4 * n_rows))
            for i, col in enumerate(categorical_cols):
                plt.subplot(n_rows, n_cols, i + 1)
                value_counts = self.data[col].value_counts().head(10)  # Top 10 valores
                value_counts.plot(kind='bar', alpha=0.7)
                plt.title(f'Distribui√ß√£o de {col}')
                plt.xlabel(col)
                plt.ylabel('Contagem')
                plt.xticks(rotation=45)

            plt.tight_layout()
            chart_name = 'distribuicoes_categoricas'
            plt.savefig(f'{charts_dir}/{chart_name}.png', dpi=300, bbox_inches='tight')
            plt.close()
            generated_charts.append(chart_name)

        # Chamar callback para registrar gr√°ficos gerados
        if chart_callback and generated_charts:
            chart_callback(generated_charts)
        elif self.chart_callback and generated_charts:
            self.chart_callback(generated_charts)

    def _generate_analysis_code(self, question: str) -> str:
        """
        Gera c√≥digo Python espec√≠fico para responder √† pergunta do usu√°rio

        Args:
            question (str): Pergunta do usu√°rio

        Returns:
            str: C√≥digo Python para an√°lise
        """
        question_lower = question.lower()

        # Obter informa√ß√µes b√°sicas do dataset
        shape = self.data.shape
        columns = list(self.data.columns)
        numeric_cols = list(self.data.select_dtypes(include=[np.number]).columns)
        categorical_cols = list(self.data.select_dtypes(include=['object']).columns)

        # Mapear tipos de perguntas para c√≥digo Python espec√≠fico
        if any(word in question_lower for word in ['tipos', 'type', 'dtype', 'dados']):
            return f"""# An√°lise de tipos de dados
print("=== AN√ÅLISE DE TIPOS DE DADOS ===")
print(f"Shape do dataset: {{data.shape}}")
print(f"Colunas: {{list(data.columns)}}")
print("\\n--- TIPOS POR CATEGORIA ---")
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
datetime_cols = data.select_dtypes(include=['datetime']).columns.tolist()

print(f"Num√©ricas ({len(numeric_cols)}): {numeric_cols[:5]}{'...' if len(numeric_cols) > 5 else ''}")
print(f"Categ√≥ricas ({len(categorical_cols)}): {categorical_cols[:5]}{'...' if len(categorical_cols) > 5 else ''}")
print(f"Data/Hora ({len(datetime_cols)}): {datetime_cols}")

print("\\n--- DETALHES DOS TIPOS ---")
for col in data.columns:
    print(f"{{col}}: {{data[col].dtype}} ({{data[col].count()}} valores v√°lidos)")
"""

        elif any(word in question_lower for word in ['correla√ß√£o', 'correla√ß√µes', 'relacionamento']):
            return f"""# An√°lise de correla√ß√µes
import seaborn as sns
import matplotlib.pyplot as plt

print("=== AN√ÅLISE DE CORRELA√á√ïES ===")
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()

if len(numeric_cols) >= 2:
    # Calcular matriz de correla√ß√£o
    corr_matrix = data[numeric_cols].corr()
    print("Matriz de Correla√ß√£o:")
    print(corr_matrix.round(3))

    # Encontrar correla√ß√µes mais fortes (> 0.7 ou < -0.7)
    strong_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            val = corr_matrix.iloc[i, j]
            if abs(val) > 0.7:
                strong_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], val))

    print("\\n--- CORRELA√á√ïES FORTES (|r| > 0.7) ---")
    for col1, col2, corr_val in strong_corr:
        print(f"{{col1}} ‚Üî {{col2}}: {{corr_val:.3f}}")

    # Gerar gr√°fico de correla√ß√£o
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f')
    plt.title('Matriz de Correla√ß√£o')
    plt.tight_layout()
    chart_name = 'matriz_correlacao'
    plt.savefig(f'charts/{chart_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"\\nGr√°fico salvo: charts/{chart_name}.png")
else:
    print("Necess√°rio pelo menos 2 colunas num√©ricas para calcular correla√ß√£o")
"""

        elif any(word in question_lower for word in ['outlier', 'outliers', 'at√≠pico', 'extremo']):
            return f"""# Detec√ß√£o de outliers
import matplotlib.pyplot as plt

print("=== DETEC√á√ÉO DE OUTLIERS ===")
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()

outlier_summary = []
for col in numeric_cols:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
    outlier_count = len(outliers)
    outlier_pct = (outlier_count / len(data)) * 100

    print(f"\\n--- {{col}} ---")
    print(f"Outliers detectados: {{outlier_count}} ({{outlier_pct:.1f}}%)")
    print(f"Limites: {{lower_bound:.2f}} a {{upper_bound:.2f}}")

    if outlier_count > 0:
        print(f"Valores outliers (amostra): {{outliers[col].head().tolist()}}")
        outlier_summary.append((col, outlier_count, outlier_pct))

# Gerar boxplot para visualizar outliers
if numeric_cols:
    fig, axes = plt.subplots(1, min(4, len(numeric_cols)), figsize=(15, 5))
    if len(numeric_cols) == 1:
        axes = [axes]

    for i, col in enumerate(numeric_cols[:4]):
        data[col].plot(kind='box', ax=axes[i])
        axes[i].set_title(f'Boxplot - {{col}}')

    plt.tight_layout()
    plt.savefig('charts/outliers_boxplot.png', dpi=300, bbox_inches='tight')
    print("\\nGr√°fico salvo: charts/outliers_boxplot.png")

print(f"\\n--- RESUMO GERAL ---")
print(f"Total de colunas analisadas: {{len(numeric_cols)}}")
print(f"Colunas com outliers: {{len(outlier_summary)}}")
"""

        elif any(word in question_lower for word in ['distribui√ß√£o', 'estat√≠stica', 'estat√≠sticas', 'resumo']):
            return f"""# Estat√≠sticas descritivas
import matplotlib.pyplot as plt

print("=== ESTAT√çSTICAS DESCRITIVAS ===")
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()

if numeric_cols:
    print("--- RESUMO ESTAT√çSTICO ---")
    desc_stats = data[numeric_cols].describe()
    print(desc_stats)

    print("\\n--- AN√ÅLISE POR COLUNA ---")
    for col in numeric_cols:
        print(f"\\n{{col}}:")
        print(f"  M√©dia: {{data[col].mean():.2f}}")
        print(f"  Mediana: {{data[col].median():.2f}}")
        print(f"  Desvio Padr√£o: {{data[col].std():.2f}}")
        print(f"  Amplitude: {{data[col].max() - data[col].min():.2f}}")
        print(f"  Coef. Varia√ß√£o: {{(data[col].std()/data[col].mean())*100:.1f}}%")

    # Gerar histogramas
    n_cols = min(len(numeric_cols), 4)
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
    if n_rows == 1:
        axes = [axes] if n_cols == 1 else axes
    else:
        axes = axes.flatten()

    for i, col in enumerate(numeric_cols):
        data[col].hist(bins=30, alpha=0.7, ax=axes[i])
        axes[i].set_title(f'Distribui√ß√£o - {{col}}')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Frequ√™ncia')

    # Remover subplots vazios
    for i in range(len(numeric_cols), len(axes)):
        fig.delaxes(axes[i])

    plt.tight_layout()
    plt.savefig('charts/distribuicoes.png', dpi=300, bbox_inches='tight')
    print("\\nGr√°fico salvo: charts/distribuicoes.png")
else:
    print("Nenhuma coluna num√©rica encontrada para estat√≠sticas")
"""

        elif any(word in question_lower for word in ['intervalo', 'm√≠nimo', 'm√°ximo', 'min', 'max', 'amplitude']):
            return f"""# An√°lise de intervalos (min/max)
print("=== AN√ÅLISE DE INTERVALOS ===")
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()

if numeric_cols:
    print("--- INTERVALOS POR VARI√ÅVEL ---")
    for col in numeric_cols:
        min_val = data[col].min()
        max_val = data[col].max()
        amplitude = max_val - min_val

        print(f"\\n{{col}}:")
        print(f"  M√≠nimo: {{min_val}}")
        print(f"  M√°ximo: {{max_val}}")
        print(f"  Amplitude: {{amplitude}}")
        print(f"  Percentil 5%: {{data[col].quantile(0.05)}}")
        print(f"  Percentil 95%: {{data[col].quantile(0.95)}}")

    # Tabela resumo
    import pandas as pd
    summary_df = pd.DataFrame({{
        'M√≠nimo': [data[col].min() for col in numeric_cols],
        'M√°ximo': [data[col].max() for col in numeric_cols],
        'Amplitude': [data[col].max() - data[col].min() for col in numeric_cols]
    }}, index=numeric_cols)

    print("\\n--- TABELA RESUMO ---")
    print(summary_df.round(2))
else:
    print("Nenhuma coluna num√©rica encontrada")
"""

        else:
            # An√°lise geral para perguntas n√£o espec√≠ficas
            return f"""# An√°lise explorat√≥ria geral
print("=== AN√ÅLISE EXPLORAT√ìRIA GERAL ===")
print(f"Dataset: {{data.shape[0]}} linhas, {{data.shape[1]}} colunas")

# Informa√ß√µes b√°sicas
print("\\n--- ESTRUTURA DOS DADOS ---")
print("Tipos de dados:")
print(data.dtypes.value_counts())

print("\\n--- VALORES AUSENTES ---")
missing = data.isnull().sum()
if missing.sum() > 0:
    print("Colunas com valores ausentes:")
    print(missing[missing > 0])
else:
    print("Nenhum valor ausente encontrado")

# An√°lise por tipo de coluna
numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

if numeric_cols:
    print("\\n--- RESUMO NUM√âRICO ---")
    print(data[numeric_cols].describe().round(2))

if categorical_cols:
    print("\\n--- AN√ÅLISE CATEG√ìRICA ---")
    for col in categorical_cols[:3]:  # Apenas primeiras 3
        print(f"\\n{{col}} - {{data[col].nunique()}} valores √∫nicos:")
        print(data[col].value_counts().head())

print(f"\\n--- CONCLUS√ÉO ---")
print(f"Dataset pronto para an√°lise com {{len(numeric_cols)}} vars num√©ricas e {{len(categorical_cols)}} categ√≥ricas")
"""

    def _execute_analysis_code(self, python_code: str) -> dict:
        """
        Executa o c√≥digo Python gerado e retorna resultados formatados

        Args:
            python_code (str): C√≥digo Python para executar

        Returns:
            dict: Resultado com output e conclus√µes
        """
        try:
            # Criar ambiente de execu√ß√£o seguro
            local_vars = {{
                'data': self.data,
                'pd': pd,
                'np': np,
                'plt': plt,
                'sns': sns
            }}

            # Capturar output
            output_buffer = io.StringIO()

            # Lista para capturar gr√°ficos gerados
            generated_charts = []

            # Fun√ß√£o mock para plt.savefig que captura nomes
            original_savefig = plt.savefig
            def capture_savefig(filename, **kwargs):
                original_savefig(filename, **kwargs)
                if isinstance(filename, str) and filename.startswith('charts/'):
                    chart_name = filename.replace('charts/', '').replace('.png', '')
                    generated_charts.append(chart_name)

            # Substituir apenas no matplotlib, n√£o no local_vars
            plt.savefig = capture_savefig

            # Executar c√≥digo
            with redirect_stdout(output_buffer):
                exec(python_code, {"__builtins__": __builtins__}, local_vars)

            # Restaurar fun√ß√£o original
            plt.savefig = original_savefig

            output = output_buffer.getvalue()

            # Registrar gr√°ficos gerados no callback
            if self.chart_callback and generated_charts:
                self.chart_callback(generated_charts)

            # Gerar conclus√µes baseadas no output
            conclusions = self._generate_conclusions_from_output(output, python_code)

            return {
                'output': output if output else "C√≥digo executado com sucesso (sem output textual)",
                'conclusions': conclusions
            }

        except Exception as e:
            return {
                'output': f"Erro na execu√ß√£o: {str(e)}",
                'conclusions': "N√£o foi poss√≠vel gerar conclus√µes devido ao erro na execu√ß√£o."
            }

    def _generate_conclusions_from_output(self, output: str, code: str) -> str:
        """
        Gera conclus√µes inteligentes baseadas no output do c√≥digo executado

        Args:
            output (str): Output da execu√ß√£o do c√≥digo
            code (str): C√≥digo Python executado

        Returns:
            str: Conclus√µes sobre os resultados
        """
        conclusions = []

        # An√°lise baseada no tipo de c√≥digo executado
        if 'tipos de dados' in code.lower() or 'dtype' in code.lower():
            numeric_count = len(self.data.select_dtypes(include=['number']).columns)
            categorical_count = len(self.data.select_dtypes(include=['object']).columns)

            conclusions.append(f"‚Ä¢ O dataset possui {{numeric_count}} vari√°veis num√©ricas e {{categorical_count}} categ√≥ricas")

            if numeric_count > categorical_count:
                conclusions.append("‚Ä¢ Predomin√¢ncia de dados num√©ricos - ideal para an√°lises estat√≠sticas")
            elif categorical_count > numeric_count:
                conclusions.append("‚Ä¢ Predomin√¢ncia de dados categ√≥ricos - adequado para an√°lises de classifica√ß√£o")

        elif 'correla√ß√£o' in code.lower():
            conclusions.append("‚Ä¢ Correla√ß√µes revelam relacionamentos lineares entre vari√°veis")
            conclusions.append("‚Ä¢ Valores pr√≥ximos de ¬±1 indicam correla√ß√£o forte")
            conclusions.append("‚Ä¢ Correla√ß√µes fortes podem indicar redund√¢ncia ou depend√™ncia entre vari√°veis")

        elif 'outlier' in code.lower():
            conclusions.append("‚Ä¢ Outliers podem representar anomalias, erros ou casos especiais")
            conclusions.append("‚Ä¢ Em detec√ß√£o de fraude, outliers s√£o especialmente importantes")
            conclusions.append("‚Ä¢ Considere investigar registros com valores extremos")

        elif 'distribui√ß√£o' in code.lower() or 'estat√≠stica' in code.lower():
            conclusions.append("‚Ä¢ Estat√≠sticas descritivas revelam padr√µes centrais dos dados")
            conclusions.append("‚Ä¢ Desvio padr√£o alto indica maior variabilidade")
            conclusions.append("‚Ä¢ Compare m√©dia vs mediana para identificar assimetria")

        elif 'intervalo' in code.lower() or 'amplitude' in code.lower():
            conclusions.append("‚Ä¢ Amplitude mostra a dispers√£o dos dados")
            conclusions.append("‚Ä¢ Intervalos muito amplos podem indicar presen√ßa de outliers")
            conclusions.append("‚Ä¢ Percentis 5% e 95% mostram distribui√ß√£o mais robusta")

        else:
            # Conclus√µes gerais
            conclusions.append(f"‚Ä¢ Dataset analisado: {self.data.shape[0]} registros, {self.data.shape[1]} vari√°veis")
            conclusions.append("‚Ä¢ An√°lise executada com sucesso usando c√≥digo Python personalizado")
            conclusions.append("‚Ä¢ Resultados baseados em c√°lculos diretos dos dados")

        return "\n".join(conclusions)

    def _analyze_data_types(self) -> str:
        """
        Analisa os tipos de dados de forma detalhada

        Returns:
            str: An√°lise detalhada dos tipos de dados
        """
        if self.data is None:
            return "Nenhum dado carregado."

        result = []
        result.append("AN√ÅLISE DE TIPOS DE DADOS")
        result.append("=" * 50)
        result.append(f"Dataset: {self.filename}")
        result.append(f"Shape: {self.data.shape[0]} linhas, {self.data.shape[1]} colunas")
        result.append("")

        # An√°lise por tipo
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns
        datetime_cols = self.data.select_dtypes(include=['datetime64']).columns

        result.append("RESUMO POR TIPO:")
        result.append(f"- Colunas numericas: {len(numeric_cols)}")
        result.append(f"- Colunas categoricas: {len(categorical_cols)}")
        result.append(f"- Colunas de data/hora: {len(datetime_cols)}")
        result.append("")

        # Detalhes das colunas num√©ricas
        if len(numeric_cols) > 0:
            result.append("COLUNAS NUMERICAS:")
            for col in numeric_cols:
                dtype = str(self.data[col].dtype)
                non_null = self.data[col].count()
                result.append(f"  - {col}: {dtype} ({non_null} valores validos)")

        # Detalhes das colunas categ√≥ricas
        if len(categorical_cols) > 0:
            result.append("")
            result.append("COLUNAS CATEGORICAS:")
            for col in categorical_cols:
                unique_vals = self.data[col].nunique()
                non_null = self.data[col].count()
                result.append(f"  - {col}: object ({non_null} valores validos, {unique_vals} unicos)")

        # Detalhes das colunas de data
        if len(datetime_cols) > 0:
            result.append("")
            result.append("COLUNAS DE DATA/HORA:")
            for col in datetime_cols:
                non_null = self.data[col].count()
                result.append(f"  - {col}: datetime64 ({non_null} valores validos)")

        # Informa√ß√µes adicionais
        result.append("")
        result.append("INFORMACOES ADICIONAIS:")
        result.append(f"- Total de valores ausentes: {self.data.isnull().sum().sum()}")
        result.append(f"- Uso de memoria: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

        return "\n".join(result)

    def get_data_summary(self) -> Dict[str, Any]:
        """
        Get a comprehensive summary of the loaded data

        Returns:
            Dict containing various data summary metrics
        """
        if self.data is None:
            return {"error": "No data loaded"}

        summary = {
            "filename": self.filename,
            "shape": self.data.shape,
            "columns": list(self.data.columns),
            "data_types": self.data.dtypes.to_dict(),
            "missing_values": self.data.isnull().sum().to_dict(),
            "numerical_columns": list(self.data.select_dtypes(include=[np.number]).columns),
            "categorical_columns": list(self.data.select_dtypes(include=['object', 'category']).columns)
        }

        # Add statistical summary for numerical columns
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            summary["statistical_summary"] = self.data[numeric_cols].describe().to_dict()

        return summary

    def set_chart_callback(self, callback):
        """Define fun√ß√£o callback para registrar gr√°ficos gerados"""
        self.chart_callback = callback

    def _store_analysis_conclusion(self, question: str, conclusion: str, output: str):
        """Armazena conclus√£o da an√°lise na mem√≥ria da sess√£o"""
        import datetime

        # Determinar tipo de an√°lise baseado na pergunta
        question_lower = question.lower()
        if any(word in question_lower for word in ['correla√ß√£o', 'correla√ß√µes', 'relacionamento']):
            analysis_type = "An√°lise de Correla√ß√£o"
        elif any(word in question_lower for word in ['outlier', 'outliers', 'at√≠pico']):
            analysis_type = "Detec√ß√£o de Outliers"
        elif any(word in question_lower for word in ['distribui√ß√£o', 'estat√≠stica', 'estat√≠sticas']):
            analysis_type = "Estat√≠sticas Descritivas"
        elif any(word in question_lower for word in ['tipos', 'type', 'estrutura']):
            analysis_type = "Estrutura dos Dados"
        else:
            analysis_type = "An√°lise Geral"

        # Extrair descobertas principais do output
        key_findings = []
        if output:
            lines = output.split('\n')
            for line in lines:
                if any(keyword in line.lower() for keyword in ['correla√ß√£o', 'outliers', 'm√©dia', 'm√°ximo', 'm√≠nimo']):
                    key_findings.append(line.strip())

        # Armazenar na mem√≥ria de conclus√µes
        conclusion_entry = {
            'timestamp': datetime.datetime.now(),
            'question': question,
            'analysis_type': analysis_type,
            'conclusion': conclusion,
            'key_findings': key_findings[:3],  # Top 3 descobertas
            'output_summary': output[:200] if output else ""  # Resumo do output
        }

        self.conclusions_memory.append(conclusion_entry)

        # Manter apenas √∫ltimas 10 conclus√µes para efici√™ncia
        if len(self.conclusions_memory) > 10:
            self.conclusions_memory = self.conclusions_memory[-10:]

        # Atualizar contexto da sess√£o
        self.session_context['analysis_count'] += 1

        # Adicionar descoberta principal ao contexto se relevante
        if conclusion and len(conclusion) > 20:
            finding_summary = conclusion[:100] + "..." if len(conclusion) > 100 else conclusion
            if finding_summary not in self.session_context['key_findings']:
                self.session_context['key_findings'].append(finding_summary)

            # Manter apenas 5 descobertas principais
            if len(self.session_context['key_findings']) > 5:
                self.session_context['key_findings'] = self.session_context['key_findings'][-5:]

        # Armazenar hist√≥rico de an√°lise
        self.analysis_history.append({
            'timestamp': datetime.datetime.now(),
            'type': analysis_type,
            'question': question,
            'success': True
        })

        # Atualizar resumo do dataset na primeira an√°lise
        if not self.session_context['dataset_summary'] and self.data is not None:
            self.session_context['dataset_summary'] = f"{self.filename}: {self.data.shape[0]} linhas, {self.data.shape[1]} colunas"

    def process_question(self, question: str) -> str:
        """
        Processa uma pergunta do usu√°rio (alias para ask_question para compatibilidade)

        Args:
            question (str): Pergunta sobre os dados

        Returns:
            str: Resposta do agente
        """
        return self.ask_question(question)

    def load_dataframe(self, dataframe: pd.DataFrame, filename: str = "dataframe_data") -> bool:
        """
        Carrega dados diretamente de um DataFrame pandas

        Args:
            dataframe (pd.DataFrame): DataFrame com os dados
            filename (str): Nome fict√≠cio para o arquivo

        Returns:
            bool: True se carregado com sucesso, False caso contr√°rio
        """
        try:
            self.data = dataframe.copy()
            self.filename = filename

            # Initialize offline analyzer
            # offline_analyzer removido

            print(f"DataFrame carregado: {self.data.shape[0]} linhas, {self.data.shape[1]} colunas")
            return True

        except Exception as e:
            print(f"Erro ao carregar DataFrame: {e}")
            return False

    def clear_memory(self):
        """Limpa toda a mem√≥ria e contexto da sess√£o"""
        try:
            # Limpar mem√≥ria do LangChain
            if hasattr(self, 'memory') and self.memory:
                self.memory.clear()

            # Limpar mem√≥ria de conclus√µes
            self.analysis_history.clear()
            self.conclusions_memory.clear()

            # Resetar contexto da sess√£o
            self.session_context = {
                'dataset_summary': None,
                'key_findings': [],
                'analysis_count': 0
            }

            print("üßπ Mem√≥ria e contexto da sess√£o limpos")

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao limpar mem√≥ria: {e}")

    def get_memory_summary(self) -> dict:
        """Retorna resumo da mem√≥ria atual"""
        return {
            'conclusions_count': len(self.conclusions_memory),
            'analysis_count': self.session_context['analysis_count'],
            'key_findings_count': len(self.session_context['key_findings']),
            'dataset_summary': self.session_context['dataset_summary'],
            'has_conversation_memory': hasattr(self, 'memory') and self.memory is not None
        }